{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2d82a-4fea-4e57-9743-1be348913e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "791437f5-363a-4d33-8ce4-b18164108925",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Проверка гипотезы по автоматизации проверки данных DataSet на предмет \"подозрительных\" выбросов занчений данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbaf1e5-50c5-44ee-8548-71bf0bb60e20",
   "metadata": {},
   "source": [
    " Мы провели графический анализ данных (см. Diploma_1). Исследование  данных в столбцах по типу box-and-wisker plot (ящик с усами) наглядно поазывают ,что в ряде столбцов ,таких как:\n",
    "Шаг нашивки                       \n",
    "Плотность нашивки                 \n",
    "модуль упругости, ГПа            \n",
    "Количество отвердителя, м.%     \n",
    "Поверхностная плотность, г/м2   \n",
    "Потребление смолы, г/м2\n",
    "имеются выбросы как в минимальной области (даже включя ноль, либо значения близкие к нулю - что противоречит физическим принципам)\n",
    "Либо значения на порядок, а то и на 3 поядка меньше медианы данных конкретного столбца так  и в максимальной области.\n",
    "\n",
    "Для принятия решения по изменению конкретных данных (замена на медиану или на умножение на конкретный коэффициент) или удалению конкретной строки данных в случае сильной флуктуации данных в строке принимаем гипотезу по созданию матрицы \"подозрительных данных\" для дальнейшей аналиа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44cc9e2-54d2-4e22-9af5-7d059c42af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем в проект требуемые библиотеки\n",
    "\n",
    "import seaborn as sns # библиотека для создания статистических графиков\n",
    "import random #  генераторатор случайных чисел и данных\n",
    "import pandas as pd # библиотека для обработки и анализа данных\n",
    "import os # библиотека функций для работы с операционной системой.\n",
    "import numpy as np\n",
    "import tensorflow as tf # фреймворк для глубокого машинного обучения\n",
    "import csv\n",
    "\n",
    "#!pip install googletrans==3.1.0a0\n",
    "#  взято из https://www.techgeekbuzz.com/blog/how-to-translate-languages-in-python/\n",
    "#!pip install googletrans==4.0.0rc1 \n",
    "\n",
    "\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "#!pip google_trans_new\n",
    "#from google_trans_new import google_translator  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib import cm\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#import plotly\n",
    "#import plotly.graph_objs as go\n",
    "#import plotly.express as px\n",
    "#from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6989912-811a-4732-8115-e92fa8447b61",
   "metadata": {},
   "source": [
    "Collecting googletrans==4.0.0rc1\n",
    "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
    "  Preparing metadata (setup.py): started\n",
    "  Preparing metadata (setup.py): finished with status 'done'\n",
    "Requirement already satisfied: httpx==0.13.3 in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from googletrans==4.0.0rc1) (0.13.3)\n",
    "Requirement already satisfied: chardet==3.* in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (3.0.4)\n",
    "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.5.0)\n",
    "Requirement already satisfied: idna==2.* in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2.10)\n",
    "Requirement already satisfied: hstspreload in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2023.1.1)\n",
    "Requirement already satisfied: certifi in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (2022.12.7)\n",
    "Requirement already satisfied: sniffio in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (1.2.0)\n",
    "Requirement already satisfied: httpcore==0.9.* in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0rc1) (0.9.1)\n",
    "Requirement already satisfied: h2==3.* in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (3.2.0)\n",
    "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (0.9.0)\n",
    "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (5.2.0)\n",
    "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\grain\\anaconda3\\envs\\myenv_jupiterlab\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0rc1) (3.0.0)\n",
    "Building wheels for collected packages: googletrans\n",
    "  Building wheel for googletrans (setup.py): started\n",
    "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
    "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17411 sha256=bc4c2f0f85c65f2d2a2e6b23c6c63a34ef82fb5b700350b263be158517ef0a76\n",
    "  Stored in directory: c:\\users\\grain\\appdata\\local\\pip\\cache\\wheels\\54\\ca\\27\\562b6eac3a495887e4b44bac3a1efe925fa603d085ba89a21d\n",
    "Successfully built googletrans\n",
    "Installing collected packages: googletrans\n",
    "  Attempting uninstall: googletrans\n",
    "    Found existing installation: googletrans 3.1.0a0\n",
    "    Uninstalling googletrans-3.1.0a0:\n",
    "      Successfully uninstalled googletrans-3.1.0a0\n",
    "Successfully installed googletrans-4.0.0rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb69af6-d6eb-432a-b3da-d83f2597ddc3",
   "metadata": {},
   "source": [
    "# Создадим матрицу(массив) \"подозрительных данных\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a136982-bd3f-42c8-9063-6abf64aacb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# теперь прочитаем наш файл формата hw_data_composite_new.CSV в  котором будем изменять часть данных в определенных строках\n",
    "\n",
    "hw_data_composite_analyzis = pd.read_csv(r'C:\\Users\\grain\\Work_folder\\Diplom_MGTU\\hw_data_composite_new.csv')\n",
    "hw_data_composite_analyzis.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616537ed-b1a3-4eec-9174-4611b42c0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# применим метод .describe() к количественным признакам\n",
    "hw_data_composite_analyzis.describe(percentiles = [0.25, 0.5, 0.75, 0.99], include='all').round(3) # дополнительно выводим еще одну квартилю 99%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0d01c-2c3c-406e-9852-5585acb259f9",
   "metadata": {},
   "source": [
    "# Проверим наш DataFrame на наличие дубликатов  строк "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adafe18-4d02-409b-9576-567553876a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_data_composite_analyzis_dupl = hw_data_composite_analyzis.loc[:,~hw_data_composite_analyzis.apply(lambda x: x.duplicated(),axis=1).all()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97921312-f6af-4dc9-902a-2d906913c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_data_composite_analyzis_dupl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01cc33-b966-441e-99b3-31e2aeb17cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_data_composite_analyzis_dupl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd874cfc-44a2-42ef-aae5-a0c2fd34d7af",
   "metadata": {},
   "source": [
    "# В нашем dataFrame НЕТ дубликатов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36c9bd-8184-47ce-a08d-ea848ae7396b",
   "metadata": {},
   "source": [
    "# Перевод  значений str из списка на другой язык (с любого )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b889c-2079-4361-ac3e-0ad696ba5aed",
   "metadata": {},
   "source": [
    "# Заменим  названия столбцов с русского языка на английский язык"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76806b04-9b4d-4a10-aaa7-10f86ef8b093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция перевода стhоковых данных ('слово или предложение') или Списка (list) на другой язык\n",
    "# передаем параметры:\n",
    "# list_data - строку в формате 'str' или в виде списка list\n",
    "# lang_old - необязательный параметр - src - код  исходного языка - по таблие сокращений языков (в нашем случае - 'ru' - русский) \n",
    "# lang_new - параметр -dest -код   целевого языка - по таблице сокращений языков (по умолчанию - 'en' - английский)\n",
    "# Если не указать lang_new - параметр -'dest' - прорамма по умолчанию применит английский язык.\n",
    "# prt - параметр для печати результатов. Если = True - печать, если Flse - Нет печати\n",
    "\n",
    "def translate_list_data (data, src, dest, prt = True):\n",
    "    \n",
    "    list_old_name = data\n",
    "    # Создаем пустой список для вывод переведенного списка\n",
    "    list_new_name = []\n",
    "\n",
    "    translator = Translator() # вызываем метод Translator()\n",
    "\n",
    "    for i in list_old_name: # проходим цикл по списку\n",
    "   \n",
    "        translated = translator.translate(i, src = 'ru', dest= 'en').text # осуществляем перевод по элемнтам списка\n",
    "    \n",
    "        list_new_name.append(translated) # добавляем переведенные элементы  в списое первода\n",
    "\n",
    "    if prt==True:\n",
    "        print(f'{list_old_name} -> {list_new_name}')\n",
    "        \n",
    "    return list_new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7bd3be-f535-42e9-aad2-7c6a3fa9c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_column =hw_data_composite_analyzis.columns.tolist() # Cписок наименований столбцов для передачи в функцию перевода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e310ec-96e9-492a-8fc4-cdac915b03df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_new_name = translate_list_data(list_column,'ru', 'en', prt = True) # Функция перевода названий столбцов ( в данном случае) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c489e9a-e580-47a1-b985-379f80a09e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23368523-c876-492a-85cc-b86f88975e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименуем наименования столбцов в нашем df\n",
    "# часть взята из https://dev-gang.ru/article/perevod-teksta-s-pomosczu-google-translate-api-v-python-ahgm88wx1k/\n",
    "\n",
    "def сhangelang_column_name(df, list_new_name, prt = True):\n",
    "    # функция полчает df  и  заменяет названия столбцов с русского языка на ангийский язык \n",
    "    # list_new_name - список новых наименований столбцов\n",
    "    # list_old_name -  Список старых названий столбцов\n",
    "  \n",
    "    list_old_name = list(df.columns)\n",
    "            \n",
    "    # Создадим словарь для его подcтановки в метод .rename для замены имен столбцов\n",
    "    dict1 = {} # создаем пустой словарь\n",
    "\n",
    "    for i in range(len(list_old_name)):\n",
    "        dict1[list_old_name[i]] = list_new_name[i]\n",
    "        df.rename(columns=dict1, inplace=True)\n",
    "\n",
    "    if prt==True:\n",
    "        print(dict1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead1e2f-beaf-4f79-8b51-b90903bd4907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "сhangelang_column_name(hw_data_composite_analyzis, list_new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee06e26-cb77-4428-a59f-8acbf76ae922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# теперь запишем наш файл с исходными данными с измененным названием столбцов на английский языке в формате .CSV \n",
    "# в нашу папку,где находится наш проектный Jupiter_notebook-Diplom_MGTU\n",
    "# и также приводим количество знаков у переменных типа float к 3 знакам после запятой float_format=\"%.3f\"\n",
    "\n",
    "hw_data_composite_analyzis.to_csv('composite_analyzis_eng.csv', index=False, float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e2bd6-6e83-4dee-ae01-4386e524967f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671c67d-576b-4d04-a2de-28563c07bc89",
   "metadata": {},
   "source": [
    "# Мы перевели названия столбцов а английский язык и заменили названия с русского на английский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab1a42-d6cc-4b69-b0fb-cbd72a81fb40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hw_data_composite_analyzis.head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f7a67-81af-4697-9b23-4bdbea78adcb",
   "metadata": {},
   "source": [
    "# замечание - в  строке с индексом 19 в столбце 'Шаг нашивки' и 'Плотность нашивки' есть значения '0.0'.\n",
    "нам необходимо провести анализ и найти 'подозрительного значения'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec4bd45-965b-458b-a2e1-392936150a9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Заметим, что '50% процентиль' тоже, что и медиана!!!\n",
    "Источник - https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.describe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1856e6-b6d9-4241-ad80-1f97d63834f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Вычислим список медиан по всем столбцам - это процентиль 50%\n",
    "\n",
    "median_composit = hw_data_composite_analyzis.describe(include='all').loc['50%']\n",
    "median_composit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e2f166-fa69-43e1-84bb-837c58388c07",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Создадим DataFrame 'median_set' из названий столбцов и значений медиан столбцов\n",
    "# Источник - https://stackoverflow.com/questions/26097916/convert-pandas-series-to-dataframe\n",
    "\n",
    "#median_set = pd.DataFrame({'Сolumns':median_composit.index, '50%-median ':median_composit.values})\n",
    "#median_set # выводим на печать"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59231b-47cb-446c-954d-548a48d1bdcb",
   "metadata": {},
   "source": [
    " new_row_median = hw_data_composite_analyzis.median(axis=0) # параметры новой строки в конце массива - это медианы столбцов!!\n",
    "\n",
    " df_zeros.append(new_row_median, ignore_index = True) # создаем новую строку медиан в конце массива # работает , строка добавляется\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea5b85-3b05-40db-8a06-a94bdc4aee52",
   "metadata": {
    "tags": []
   },
   "source": [
    " значение критериев 'min_data'  - все значения меньше 10% от медианы- 'подозрительного значения'\n",
    " значение критериев 'max_data'  - все значения больше медианы в 10 раз  - 'подозрительного значения'\n",
    "min_data = 0.1\n",
    "max_data = 10.0\n",
    " \n",
    " переводим список медиан в DataFrame\n",
    " df_median = pd.DataFrame({'Сolumns':median_composit.index, '50%-median ':median_composit.values}) \n",
    "\n",
    "for column in hw_data_composite_analyzis.columns: #\n",
    "    for i in range(len(hw_data_composite_analyzis)):\n",
    "        \n",
    "        value = hw_data_composite_analyzis.at[i,column] # значение из df -  hw_data_composite_analyzis ВАЖНО присвоение .at[i,column]\n",
    "        min_median = min_data * median_composit[column] # значение критерия 'min_median' = критерий min * медиану столбца\n",
    "        max_median = max_data * median_composit[column] # значение критерия 'mvax_median' = критерий max * медиану столбца\n",
    "               \n",
    "        if value < min_median  or value > max_median: # условие применения критериев\n",
    "            df_zeros.at[i,column] = value  # запись 'подозрительного значения' в соотвествующую ячейку нолевой матрицы df_zeros\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b069b0-d996-4067-9274-f9735483cc96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Создадим строку медиан из hw_data_composite_analyzis и добавим ее в конце массива df_zeros!!!#new_row_median = hw_data_composite_analyzis.median(axis=0) # параметры новой строк в конце массива!!\n",
    "#df_zeros.append(new_row_median, ignore_index = True) # создаем новую строку медиан\n",
    "\n",
    " \n",
    "# pd.concat([df2, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e706f5c-c497-4731-a4dd-c7e5ac23a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_dataset(df, min_data, max_data, median_composit):\n",
    "# Функция analyzis_dataset создает массив 'подозрительных значений' в матрице заполненой NaN df_zeros\n",
    "# Передем в функцию:\n",
    "# df который будем исследовать\n",
    "# min_data - минимальный коэффициент отсечки для выбросов значений \"слева\"\n",
    "# max_data - максимальный коэффициент отсечки для выбросов значений \"справа\"\n",
    "# значение критериев 'min_data'  - все значения меньше 10% от медианы столбца - 'подозрительное значение'\n",
    "# значение критериев 'max_data'  - все значения больше медианы столбца в 10 раз  - 'подозрительное значение'\n",
    "# принимаем min_data = 0.1\n",
    "#           max_data = 10.0\n",
    "# median_composit - список median массива df рассчитан ранее в программе\n",
    " \n",
    "    list_df = list(df) # получаем список наименований столбцов из df\n",
    "    \n",
    "    # создаем df_zero заполненый 'NaN'  размерности  df (по умолчанию - с данными типа object)\n",
    "    df_zeros = pd.DataFrame(columns=list_df, index = range(0, len(df)))\n",
    "                      \n",
    "    for column in df.columns: # иттерация по столбцам\n",
    "        for i in range(len(df)): # иттерация по строкам\n",
    "            \n",
    "            value = df.at[i,column] # значение из df -  hw_data_composite_analyzis ВАЖНО писвоение .at[i,column]\n",
    "        \n",
    "            min_median = min_data * median_composit[column] # значение критерия 'min_median' = критерий min * медиану столбца\n",
    "            max_median = max_data * median_composit[column] # значение критерия 'mvax_median' = критерий max * медиану столбца\n",
    "               \n",
    "            if value < min_median  or value > max_median: # условие применения критериев\n",
    "                df_zeros.at[i,column] = value  # запись 'подозрительного значения' в соотвествующую ячейку нолевой матрицы df_zeros  \n",
    "        \n",
    "    # меняем тип данных  массива на float32                       \n",
    "    df_zeros = df_zeros.astype(np.float32)\n",
    "    \n",
    "    return df_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895cd03-9b3a-465c-8d0d-a3206e62d02f",
   "metadata": {},
   "source": [
    "# Необходимо создать  отдельный модуль - функцию для проверки нашего DataSet 'hw_data_composite_analyzis' на 'подозрительные значения' и создание массива 'подозрительных значений'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162dbdf-5a64-452e-a3f0-8d7942e1829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_dataset(df, min_data, max_data, median_composit):\n",
    "# Функция analyzis_dataset создает массив 'подозрительных значений' в матрице заполненой нулями df_zeros\n",
    "# Передем в функцию:\n",
    "# df который будем исследовать\n",
    "# min_data - минимальный коэффициент отсечки для выбросов значений \"слева\"\n",
    "# max_data - максимальный коэффициент отсечки для выбросов значений \"справа\"\n",
    "# значение критериев 'min_data'  - все значения меньше 10% от медианы- 'подозрительного значения'\n",
    "# значение критериев 'max_data'  - все значения больше медианы в 10 раз  - 'подозрительного значения'\n",
    "# принимаем min_data = 0.1\n",
    "#           max_data = 10.0\n",
    "# median_composit - список median массива df рассчитан ранее в программе\n",
    "\n",
    "# создаем df_zero заполненый 'NaN'  размерности  df ( по умолчанию - с данными типа float64)\n",
    "# list(df.columns) - это список наименований столбцов df (list)\n",
    "# количество строк - len(df) \n",
    "    \n",
    "    df_zeros = pd.DataFrame(columns = list(df.columns), index = range(0, len(df))) \n",
    "\n",
    "    for column in df.columns: # иттерация по столбцам\n",
    "        for i in range(len(df)): # иттерация по строкам\n",
    "            \n",
    "            value = df.at[i,column] # значение из df -  hw_data_composite_analyzis ВАЖНО писвоение .at[i,column]\n",
    "        \n",
    "            min_median = min_data * median_composit[column] # значение критерия 'min_median' = критерий min * медиану столбца\n",
    "            max_median = max_data * median_composit[column] # значение критерия 'mvax_median' = критерий max * медиану столбца\n",
    "               \n",
    "            if value < min_median  or value > max_median: # условие применения критериев\n",
    "                df_zeros.at[i,column] = value  # запись 'подозрительного значения' в соотвествующую ячейку нолевой матрицы df_zeros  \n",
    "                \n",
    "    df_zeros = df_zeros.astype(np.float32)\n",
    "    return df_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216f909-6c5a-40ec-b927-fe56bd60b96a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# проверка функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac5306-d0aa-4b3f-a5f2-6d3c94a449c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_zeros = analysis_dataset(hw_data_composite_analyzis, 0.1, 10, median_composit)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a701e-0bc7-4ab8-bcd2-98747578a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zeros.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980820b6-4aeb-4b4a-a425-28cbc4e90301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_zeros.head(21).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2c8bc-cd3c-4f83-989c-2daa9e0ad2b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "как видим в строке с индексом 19 в первом и втором столбцах появились значения- маркеры- 0.01 - значит в этих ячейках были аномальные значения '0.0'. в других строках в других таблицах также стоят значения отличающиеся от ьедианы столбца. Например в строке 104 в столбце \"модуль упругости, ГПа\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b4e2f-b7cb-4093-ad0c-8955f3407531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_zeros.iloc[100:110, 1 : 12].round(3) # !!! вывод необходимых строк"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c7930-4517-4020-8340-0645f5caa3be",
   "metadata": {},
   "source": [
    "# у нас программа нашла, для примера 'подоззрительное значение' в строке 109 столбец 'модуль упругости, ГПа' = 9.986\n",
    "и в строке 109 столбца 'Поверхностная плотность, г/м2' = 12.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bd76f-9a87-409b-9dab-37aac9ca053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zeros.nunique(axis= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417c210-17f0-45f3-a614-380eebed99b8",
   "metadata": {},
   "source": [
    "# Судя по данным .nunique(axis= 0) у нас всего 70 подозрительных значений в 4 колонках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d879e2-b086-4ec6-9325-13d3c4ae50f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# копируем df_zeros с ошибками в новый df и будем работать дальше с df_zeros_dropNaN\n",
    "df_zeros_dropNaN = df_zeros.copy()\n",
    "df_zeros_dropNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8dd0e-9cc1-4784-b13a-315e157c868b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Удаляем все строки в которых содержаться исключительно значения NaN\n",
    "# Мы получим df_analises в котором индексы укажут на строки в которых есть bugs!!!!!\n",
    "\n",
    "df_analises = df_zeros_dropNaN.dropna(axis=0, how='all')\n",
    "\n",
    "df_analises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c474d-f55b-4d84-b62a-5aa1c8f31460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analises.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b0d72-3d63-4830-92ba-7653467a4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем столбцы , где все значения NaN\n",
    "df_analises.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a735a-4824-4df4-b01f-9b030a34e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# список индексов ,в которых есть  bugs. Таких строк  - 66!!!\n",
    "list_id_bugs = list(df_analises.index) \n",
    "print(list_id_bugs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e4d62-fc4f-455c-b828-e21c625b0343",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Рассчитаем количество строк в которых есть 'подозрительные значения' в массиве df_zeros.\n",
    "Создадим массив строк df_analiz используя массив df_zeros, сохранив их оригинальные индексы в отдельном столбце и введя особый столбец с указнием количества 'подозрительных значений'\n",
    "Если в какой-то строке будет больше 1 'подозрительноо значения', то  скорее всего мы будем удалять эту строку в нашем рабочем файле hw_data_composite_analyzis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3b704-7c0a-4dbf-bef7-12c9a93123f8",
   "metadata": {},
   "source": [
    "Мы можем использовать метод .rename() для переименования определенных или всех столбцов с помощью словаря. Нам не нужны круглые скобки, поэтому давайте избавимся от них:\n",
    "\n",
    "!!!Если вы будете работать с набором данных в течение некоторого времени, рекомендуется использовать нижний регистр, удалить специальные символы и заменить пробелы символами подчеркивания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9bae3-5ac3-4b8f-9088-c120d9213c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c72f4b-9713-4e67-96c3-69a5512152f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99054b-d73a-4dfa-bdff-5ddece346c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
