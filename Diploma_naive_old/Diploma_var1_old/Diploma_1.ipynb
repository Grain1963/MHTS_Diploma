{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4044810-6810-4378-85da-f0377635c858",
   "metadata": {},
   "source": [
    "Дипломная работа на курсе МГТУ им. Н. Баумана\n",
    "\n",
    "Проект по разработке нейронной сети по определению характеристик компазиционных материалов\n",
    "Версия 1.\n",
    "Начало."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e092e09-fd54-43c5-8aef-6bdee2af2485",
   "metadata": {},
   "source": [
    "Подготовка среды для проекта ENV\n",
    "При начале работы над проектом необходимо было установить несколько пакетов библиотек Python таких как tensorflow, seaborn и др.\n",
    "Выяснилось, что невозможно установить пакеты через PIP install, так как win 10 не воспринимат путь к файлу длиннее 260 симвлов. Пришлось написть программу по изменению Политики доступа для win 10 с расширением .bat. Однако это тоже не дало результатов. Пришлось с помощью изменения regedit вносить изменения в ключь Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem, что привело к возможности записывать блинные пути к файлу."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10dba6-730c-4b57-a3e4-9169deb0f2b5",
   "metadata": {},
   "source": [
    "Но! Установка tensorflow прошла только для моего окружения  MeEnvJupiterLab!!!!!  \n",
    "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\grain\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n",
    "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
    "Successfully installed tensorflow-2.11.0 tensorflow-intel-2.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5923f-036d-4d74-9b2d-e94aa7826a01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9f178-76be-4b07-a4e1-946cddfd4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем в проект требуемые библиотеки\n",
    "\n",
    "import seaborn as sns # библиотека для создания статистических графиков\n",
    "import random #  генераторатор случайных чисел и данных\n",
    "import pandas as pd # библиотека для обработки и анализа данных\n",
    "import os # библиотека функций для работы с операционной системой.\n",
    "import numpy as np\n",
    "import tensorflow as tf # фреймворк для глубокого машинного обучения\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#!pip install jupyterlab-kite\n",
    "\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "# функции взятые со стороны\n",
    "#from my_modul import func_import_data_from_csv  as fidf_csv # Load a dataframe from csv-file and optimize its memory usage.\n",
    "    # (RU) Загрузка данных из csv-файла и оптимизация числовых типов для оптимизации использования памяти\n",
    "    \n",
    "# from my_modul import func_optimize_memory_usage as fomu_csv # Function optimizes memory usage in dataframe. (RU) Функция оптимизации типов в dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914b722-99aa-4b42-abed-57ed321c4d57",
   "metadata": {},
   "source": [
    "CRISP-DM – Cross-Industry Standard Process for Data Mining\n",
    "CRISP-DM описывает жизненный цикл исследования данных, состоящий из 6 фаз/этапов.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38d2d9-10cf-49b6-9a33-04070a852d68",
   "metadata": {},
   "source": [
    "# Этап 1 Business Understanding / Бизнес анализ\n",
    "\n",
    "Композиционные материалы - это искусственно созданные материалы, состоящие из нескольких других с четкой границей между ними. Композиты обладают теми свойствами, которые не наблюдаются у компонентов по отдельности. При этом композиты являются монолитным материалом, т.е. компоненты материала неотделимы друг от друга без разрушения конструкции в целом. Яркий пример композита - железобетон. Бетон прекрасно сопротивляется сжатию, но плохо растяжению. Стальная арматура внутри бетона компенсирует его неспособность сопротивляться сжатию, формируя тем самым новые, уникальные свойства. Современные композиты изготавливаются из других материалов: полимеры, керамика, стеклянные и углеродные волокна, но данный принцип сохраняется. У такого подхода есть и недостаток: даже если мы знаем характеристики исходных компонентов, определить характеристики композита, состоящего из этих компонентов, достаточно проблематично. Для решения этой проблемы есть два пути: физические испытания образцов материалов, или прогнозирование характеристик. Суть прогнозирования заключается в симуляции представительного элемента объема композита, на основе данных о характеристиках входящих компонентов (связующего и армирующего компонента).\n",
    "На входе имеются данные о начальных свойствах компонентов композиционных материалов (количество связующего, наполнителя, температурный режим отверждения и т.д.). На выходе необходимо спрогнозировать ряд конечных свойств получаемых композиционных материалов. Кейс основан на реальных производственных задачах Центра НТИ «Цифровое материаловедение: новые материалы и вещества» (структурное подразделение МГТУ им. Н.Э. Баумана).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cca58a-0cdf-4947-ac97-676e82550535",
   "metadata": {},
   "source": [
    "# Этап 2 Data Understanding / Анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e2427-03c5-4593-a60b-23015c434df0",
   "metadata": {},
   "source": [
    "# Этап 2.1. Collect Initial Data/ Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65551f2e-bee9-412c-abda-9a2e76728a4a",
   "metadata": {},
   "source": [
    "Загружаем DataSet предложеный  на курсе (он состоит из 2-х файлов) по ссылке:\n",
    "https://drive.google.com/file/d/1B1s5gBlvgU81H9GGolLQVw_SOi-vyNf2/view?usp=sharing\n",
    "При распковки DataSet получались 2 файла формата excel. При открытии этих файлов в JupiterLab выдается ошибка:\n",
    "File Load Error for X_nup.xlsx\n",
    "C:\\Users\\grain\\Work_folder\\Diplom_MGTU\\data_diplom\\X_nup.xlsx is not UTF-8 encoded\n",
    "Открыв файлы в программе Excel попытался записать исходные файлы с расширением .csv в кодировке \"UTF-8, разделитель запятая\".\n",
    "При попытке прочтения этих файлов с расширением .csv в коде в Jupiter df_bp = pd.read_csv(r'C:\\Users\\grain\\Work_folder\\Diplom_MGTU\\x_bp.csv') все равно получается ошибка:\n",
    "ParserError: Error tokenizing data. C error: Expected 10 fields in line 25, saw 11\n",
    "Тогда я прочитал DataSet в Jupiternotebook как файл Excel: df_bp = pd.read_excel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df82d20-70c6-44b4-8def-37dd89b8c25d",
   "metadata": {},
   "source": [
    "# Этап 2.2. Discribe Data / Описание данных и Verify Data Quality / Проверка качества данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab7519-9891-4755-b82f-56ac6cf53bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# считываем 1-й файл X_bp.xlsx\n",
    "df_bp = pd.read_excel(r'C:\\Users\\grain\\Work_folder\\Diplom_MGTU\\data_diplom\\X_bp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c253b67-eb35-4695-860b-0572a57c0135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bp # выводим df_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930660ef-1aa4-48ac-b0e8-4997e13233d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53493bf-9c76-4bfc-b095-e7e2a1af9cc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Файл Excel прочитался, во всех колонках после запятой 6 знаков и! появился новый столбец с именем Unname 0:\n",
    "(которую мы должны убрать - займемся этим вопросом позже)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e1964-fa65-4801-98ee-c37772cc2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем вновь появившийся столбец \"Unnamed: 0\"\n",
    "# ['Unnamed: 0'] - указывает на столбец ,который нужно удалить\n",
    "# axis=1 - Помечает столбец в фрейме данных, подлежащие удалению\n",
    "# inplace=True - Выполняет операцию удаления в том же фрейме данных, \n",
    "# а не создает новый объект фрейма данных во время операции удаления.\n",
    "\n",
    "df_bp.drop(['Unnamed: 0'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d0372-ee31-4b39-be17-af75c91879fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece77d94-1fcf-4149-9aed-0356a201dc84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# теперь запишем наш файл с исходными данными уже в формате .CSV в нашу папку, \n",
    "# где находится наш проектный Jupiter_notebook-Diplom_MGTU\n",
    "# Приводим количество знаков у переменных типа float\n",
    "# к 3 знакам после запятой float_format=\"%.3f\" для единообразия\n",
    "\n",
    "df_bp.to_csv('x_bp_new.csv', index=False, float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81327b-aebc-49b3-a911-fb807b284e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь прочитаем нормальный файл формата .CSV\n",
    "\n",
    "df_bp_new = pd.read_csv(r'C:\\Users\\grain\\Work_folder\\Diplom_MGTU\\x_bp_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b876d8-afa9-4cac-9a92-1378baf115a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .head() выводим первые 5 строк для проверки.\n",
    "\n",
    "df_bp_new\n",
    "\n",
    "# Как видим у разных колонок разное количество знаков после запятой. Мы займемся этим на этапе подготовки данных для машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab404560-1eed-4475-bef6-6bbdbdeb49e4",
   "metadata": {},
   "source": [
    "Проделаем ниже такие же манипуляции со вторым файлом из DataSet из папки Diplom_MGTU/data_diplom/X_nup.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad63033-7c0b-4d8d-91e8-08ca16e0ef74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# считываем 2-й файл X_nup.xlsx\n",
    "\n",
    "df_nup = pd.read_excel(r'C:\\Users\\grain\\Work_folder\\Diplom_MGTU\\data_diplom\\X_nup.xlsx')\n",
    "df_nup.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e95027-5d59-4628-b749-ef124f6c4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем вновь появившиеся столбцы\n",
    "# Не забываем про , inplace=True\n",
    "\n",
    "df_nup.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a9ddc-7e2c-4f75-a1e2-5a7c96d82109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# теперь запишем наш файл с исходными данными уже в формате .CSV в нашу папку, \n",
    "# где находится наш проектный Jupiter_notebook-Diplom_MGTU\n",
    "# и также приводим количес тво знаков у переменных типа float к 3 знакам после запятой float_format=\"%.3f\"\n",
    "\n",
    "df_nup.to_csv('x_nup_new.csv', index=False, float_format=\"%.3f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a4c45-cfaf-4e8a-894a-9cf51eae3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь прочитаем нормальный файл формата .CSV\n",
    "\n",
    "df_nup_new = pd.read_csv(r'C:\\Users\\grain\\Work_folder\\Diplom_MGTU\\x_nup_new.csv')\n",
    "df_nup_new.tail()\n",
    "\n",
    "# .tail() выводим 5 последних строк для проверки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa34dd-6676-4a46-a311-653125303bc6",
   "metadata": {},
   "source": [
    "Таким образом, мы получили 2 файла из нашего исходного DataSet:\n",
    "df_bp_new.csv (1023 строк на 10 столбцов)\n",
    "df_nup_new.csv (1040 строк на 3 столбца)\n",
    "Переходим к следующему Этапу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4562fd-c337-4645-bc86-bf3e7c8c9c3c",
   "metadata": {},
   "source": [
    "# Этапа 3 Data Preparetion / Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df589369-0f06-497f-9b2d-ee41e56fe22d",
   "metadata": {},
   "source": [
    "# Этап 3.1. Select Data / Выборка данных\n",
    "\n",
    "Два полученных нами файла с данными указывают, что файл df_bp_new.csv (1023 строк на 10 столбцов) является базовым файлом и имет 1023 строки данных по характеристикам различных композитных материалов.\n",
    "А файл df_nup_new.csv (1040 строк на 3 столбца) имеет на 17 строк больше чем основной файл df_bp_new.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c77876-b920-4f03-90a0-47345e83436c",
   "metadata": {},
   "source": [
    "# Этап 3.2. Clean Data/ Очистку данных\n",
    "\n",
    "Убираем из файла df_nup_new.csv  лишние 17 строк.\n",
    "\n",
    "name.index - список строк, подлежащих удалению в файле\n",
    "axis=0 - Помечает строки в фрейме данных, подлежащие удалению\n",
    "inplace=True - Выполняет операцию удаления в том же фрейме данных, а не создает новый объект фрейма данных во время операции удаления.\n",
    "\n",
    "df.index[1023:1040] генерирует диапазон строк от 1023 до 1040 (у нас 1039 +1 чтобы вошла и последняя строка). Нижний предел диапазона является включающим, а верхний предел диапазона является исключающим. Это означает, что строки 1023 и 1039 будут удалены.\n",
    "inplace=True выполняет операцию удаления в том же фрейме данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6959bc8-b30e-4599-8eaa-102c2d35b53e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Удаляем из датасета 17 строк, в которых недостаточно данных для обучения.\n",
    "df_nup_new.drop(df_nup_new.index[1023:1040], axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44218f32-aa33-450b-bba9-f55f0e8286fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_nup_new.tail() # проверяем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf8b50-66ff-4d3a-8d80-32bad395ead0",
   "metadata": {
    "tags": []
   },
   "source": [
    "У нас получилась таблица 1023 строки на 3 стобца\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891e928-cf0f-4825-9e68-5398aedc7a24",
   "metadata": {},
   "source": [
    "# Этап 3.3. Integrate Data / Интеграция файлов\n",
    "\n",
    "Объеденяем 2 файла в один hw_data_composite с количесвом строк 1023  и добавляем к имеющимся в файле df_bp_new.csv 10 столбцам  еще 3 столбца  из файла df_nup_new.csv.\n",
    "В итоге должны получимть DataSen размерностью 1023 строк на 13 столбцов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa89ed-9814-4456-a221-22ccffb4ece9",
   "metadata": {},
   "source": [
    "Получим информацию о наших файлах df_bp_new.csv и df_nup_new.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a96e76-9f4a-4e71-9497-cb531b0cfa83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Получаем информацибю о df_bp_new\n",
    "df_bp_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b7b8d-db5a-4cfd-9fb5-3ec777a3d44d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Получаем информацибю о df_nup_new\n",
    "df_nup_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e27512-b97a-43fe-a324-1f4b01bbc49d",
   "metadata": {},
   "source": [
    "Для уменьшения размеров используемой памяти изменим размерность float64 на float32 а также Int64 на int32b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce95cb-53d4-4749-96e0-4c8ff915f6b7",
   "metadata": {},
   "source": [
    "!!!! Код взят из источника https://github.com/ellavs/python-pandas-optimize-dataframe-memory-usage\n",
    "Для статьи: https://www.e-du.ru/2021/10/optimize-dataframe-memory-usage.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a0e69e-bd5c-46d9-8812-1d08ccbaa465",
   "metadata": {},
   "source": [
    "# ЗАДАЧА перевести нижние 2 функции в модули и запрашивать их из основной программы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa22ca0-bb39-4370-8e9c-7efde875a010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !!!! Код взят из источника https://github.com/ellavs/python-pandas-optimize-dataframe-memory-usage\n",
    "# Function optimizes memory usage in dataframe.\n",
    "# (RU) Функция оптимизации типов в dataframe.\n",
    "\n",
    "def optimize_memory_usage(df, print_size=True):\n",
    "\n",
    "\n",
    "# Types for optimization.\n",
    "    # Типы, которые будем проверять на оптимизацию.\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    # Memory usage size before optimize (Mb).\n",
    "    # (RU) Размер занимаемой памяти до оптимизации (в Мб).\n",
    "    before_size = df.memory_usage().sum() / 1024**2    \n",
    "    for column in df.columns:\n",
    "        column_type = df[column].dtypes\n",
    "        if column_type in numerics:\n",
    "            column_min = df[column].min()\n",
    "            column_max = df[column].max()\n",
    "            if str(column_type).startswith('int'):\n",
    "                if column_min > np.iinfo(np.int32).min and column_max < np.iinfo(np.int32).max:\n",
    "                    df[column] = df[column].astype(np.int32)\n",
    "                elif column_min > np.iinfo(np.int64).min and column_max < np.iinfo(np.int64).max:\n",
    "                    df[column] = df[column].astype(np.int64)  \n",
    "            else:\n",
    "                if column_min > np.finfo(np.float32).min and column_max < np.finfo(np.float32).max:\n",
    "                    df[column] = df[column].astype(np.float32)\n",
    "                else:\n",
    "                    df[column] = df[column].astype(np.float64)    \n",
    "    # Memory usage size after optimize (Mb).\n",
    "    # (RU) Размер занимаемой памяти после оптимизации (в Мб).\n",
    "    after_size = df.memory_usage().sum() / 1024**2\n",
    "    if print_size: print('Memory usage size: before {:5.4f} Mb - after {:5.4f} Mb ({:.1f}%).'.format(before_size, after_size, 100 * (before_size - after_size) / before_size))\n",
    "    return df\n",
    "\n",
    "def import_data_from_csv(df):\n",
    "    # Show dataframe info before optimize.\n",
    "    # (RU) Показать информацию о таблице до оптимизации.\n",
    "    print('-' * 80)\n",
    "    print(df.info())\n",
    "    print('-' * 80)\n",
    "    # (RU) Оптимизация типов в dataframe.\n",
    "    df = optimize_memory_usage(df) # используем функцию optimize_memory_usage , созданую выше\n",
    "    # Show dataframe info after optimize.\n",
    "    # (RU) Показать информацию о таблице после оптимизации.\n",
    "    print('-' * 80)\n",
    "    print(df.info())\n",
    "    print('-' * 80)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afcc15-4710-4c3e-881d-6b506c2ae64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вызываем функцию import_data_from_csv для уменьшения размерности переменных в файле df_bp_new\n",
    "\n",
    "df_bp_new = import_data_from_csv(df_bp_new) # проверяем, размерность изменилась"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad229ee7-1773-4b3e-bf96-aefe0e6cedf2",
   "metadata": {},
   "source": [
    "Понизив размерность данных в файле df_bp_new с float64 до float32 мы уменьшили размер используемой памяти с 78,2 кВ до 39,1 кВ, т.е на 49,9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b070a-7138-4c3d-be54-26a07958debe",
   "metadata": {},
   "source": [
    "В файле df_nup_new также уменьшим размерность, вызвав функцию import_data_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e493dd-f51f-4f3c-b3a5-01a50e1ffec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nup_new = import_data_from_csv(df_nup_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1027c80-92d3-4fe4-8f9a-63821a8635a5",
   "metadata": {},
   "source": [
    "Понизив размерность данных в файле df_nup_new с float64 до float32, и с int64 до in32 мы уменьшили размер используемой памяти с 23,5 кВ до 11,8 т.е на 49,7%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74b7ad5-9c4d-4b72-8b85-56395f3b7131",
   "metadata": {},
   "source": [
    "# Этап 3.3. Integrate Data  / Интеграция данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87c092-4f2f-4fe5-ba5a-10b91edd9543",
   "metadata": {},
   "source": [
    "Объеденяем с помощью pd.merge по способу how='inner' 2 начальных файла в один файл hw_data_composite с количесвом строк 1023  и добавляем к имеющимся в файле df_bp_new.csv 10 стоaлбцам  еще 3 столбца  из файла df_nup_new.csv.\n",
    "В итоге должны получимть DataSet размерностью 1023 строк на 13 столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99cc2c-2b09-4337-ba8a-49a1d0a38fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hw_data_composite = pd.merge(df_nup_new, df_bp_new, how='inner', on=None, left_on=None, right_on=None,\n",
    "                             left_index=True, right_index=True, sort=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e694632-ea6b-4558-8c5f-6a7dd244e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем что получилось\n",
    "hw_data_composite # итоговый DataFrame с размерностью  1023 на 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24cf130-b526-43ce-ad10-a00a2d461cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hw_data_composite.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a7159-591c-49e4-a7d0-79364b370e62",
   "metadata": {
    "tags": []
   },
   "source": [
    "Размер занятой памяти нашего массива составил 52,1 КВ, данные в размерности 32\n",
    "Изучаем характеристики этого DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90c8bf-66d5-48a0-893c-1db164ef17ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# !!! Cохраним основной DataFrame в файле hw_data_composit.csv на диске в директории Diplom_MGTU чтобы не портить начальные данные. Дальнейшие процедуры будем проводить в новом ноутбуке Diploma_2_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e972e-1d48-49b6-891a-c90047369255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hw_data_composite.to_csv('hw_data_composite.csv', index=False, float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338327f-d100-4b71-bb6e-a8dd5df0a5d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Этап 3.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e96c2a-4f95-4217-9512-5b9c67394354",
   "metadata": {},
   "source": [
    "# Проверка того, есть ли в структуре данных какие-либо пропущенные значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3957acf-57ef-4f81-9ae3-004e8ccb5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# метод .isnull() выдает логический массив, где пропуски обозначены как True.\n",
    "# Функция .isnull() используется для проверки того, есть ли в структуре данных какие-либо пропущенные значения.\n",
    "# метод .sum() по умолчанию суммирует эти True или единицы по столбцам (axis = 0)\n",
    "# Можно использовать функцию .isnull() вместе с .sum(), чтобы увидеть количество пропущенных значений в каждом столбце.\n",
    "\n",
    "hw_data_composite.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02e758-9907-4e77-a24b-d6e3d4e81004",
   "metadata": {},
   "source": [
    "# Пропущенных значений нет!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906cabd-3d70-451c-a346-4544faf4bcab",
   "metadata": {},
   "source": [
    "# Проверка того, есть ли в структуре данных какие-либо отсутствующих значений (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46453b-9655-43f7-ad7d-102cf05e6a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция .isna() в Pandas используется для обнаружения отсутствующих значений (NaN), \n",
    "# значения NaN в структуре данных сопоставляются с True, а значения, отличные от NaN, сопоставляются с False.\n",
    "# Можно использовать функцию isna вместе с sum, чтобы увидеть количество пропущенных значений в каждом столбце.\n",
    "hw_data_composite.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a723cc19-384a-45b0-b996-494b0fd5726e",
   "metadata": {},
   "source": [
    "# Мы определили, что в нашем DataFrame hw_data_composite нет  пропущенных значений и отсутствующих значений (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e42612-f107-4228-aae7-074c1f866faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# применим метод .describe() к количественным признакам\n",
    "# дополнительно выводим еще одну квартилю 99% - 0.99\n",
    "# округлим значения до3-х знаков после запятой - round(3)\n",
    "hw_data_composite.describe(percentiles = [0.25, 0.5, 0.75, 0.99], include='all').round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2298463-1fbc-44da-bdab-4915a272fdc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# При анализе hw_data_composite.describe заметим , что в столбце 'Угол нашивки, град' 50% количества значений имеют цифру '0.0', а другие 50% имеют значение '90.0'.\n",
    "Этот столбец можно удалить ,так как он не несет смысловой нагрузки ,что приведет к уменьшению количества признаков для дальнейшего обучения и работы машинного обучения и нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6f8c6-292d-4853-bf75-38e9e01dcff3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "JUr9QwMwaGVg",
    "outputId": "f34b06ea-17b2-4dbc-c8a4-8941ff7f48ff"
   },
   "outputs": [],
   "source": [
    "# копирование датафрейма hw_data_composite в hw_data_composite_new, чтобы дальнейшими действиями не внести случайных изменений\n",
    "hw_data_composite_new = hw_data_composite.copy()\n",
    "hw_data_composite_new.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a43fa0-c210-497b-bd6e-dcb5798e2b1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "pBcjXKBEaF9H",
    "outputId": "c19f9de0-d719-4b73-b795-76a593e050d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# удаление столбца 'Угол нашивки, град' )\n",
    "hw_data_composite_new.drop(['Угол нашивки, град'], axis=1, inplace=True)\n",
    "hw_data_composite_new.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516b9d6-0352-4945-8b53-fd247c4beb1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hw_data_composite_new.to_csv('hw_data_composite_new.csv', index=False, float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f52075-1a68-456e-ab7c-e65c34a74a56",
   "metadata": {},
   "source": [
    "# Проведем графический анализ имеющихся данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366cf0c-9d8d-4f8b-927c-ca0528778731",
   "metadata": {},
   "source": [
    "# Гистограммный анализ\n",
    "Используем библиотеку seaborn для построения гистограммы.\n",
    "Мы делим наши данные на интервалы (bins) и считаем, сколько наблюдений попало в каждый из них.\n",
    "Построим гистограммы для каждого параметра."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c4daf-76b4-4579-96f6-a243719522ce",
   "metadata": {},
   "source": [
    "# Необходимо перевести следующую функцию в модуль и вызывать  ее из модуля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13229383-cf1e-41bc-9f72-20e9eb7579e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в библиотеке Seaborn sns.histplot мы указываем источник данных, что будет на оси x и количество интервалов\n",
    "# Создадим цикл для прорисовки гистограм для всех столбцов\n",
    "# параметр kde = True добавляет кривую плотности распределения\n",
    "# параметр bin_s - задаем - наглядное значение- 20!!! интервалы (bins) и считаем, сколько наблюдений попало в каждый из них.\n",
    "\n",
    "def plt_hist(df, name_type_hist, bin_s):\n",
    "# Функция печати гистограмм по столбцам dataframe для анализа\n",
    "# передаем в функцию df имя dataframe, name_type_hist (либо sns.histplot, либо sns.displot или sns.boxplot) \n",
    "# наименование типа функции для построения типа гистограммы,\n",
    "# занчение bin_s = для указания количества выбор для гистаграммы\n",
    "\n",
    "    if name_type_hist == sns.histplot:\n",
    "        print('     Исследование гистограмм данных в столбцах от количества измерений  ')\n",
    "        print('-' * 80)\n",
    "        cols = df.columns\n",
    "        for column in cols:  \n",
    "            print('Гистограмма: ', column )\n",
    "            name_type_hist(data =df, x = column, bins = bin_s, kde = True, fill = True)\n",
    "            plt.show()# выводим отдельно каждый график\n",
    "        \n",
    "    elif name_type_hist == sns.displot:\n",
    "        print('     Исследование гистограмм данных в столбцах от плотности непрерывного случайного распределения   ')\n",
    "        print('-' * 80)\n",
    "        cols = df.columns\n",
    "        for column in cols:  \n",
    "            print('Гистограмма: ', column )\n",
    "            name_type_hist(data =df, x = column, kind = 'kde', fill = True) #bins = bin_s # hue=\"cut\"\n",
    "            plt.show()\n",
    "    else:\n",
    "        name_type_hist == sns.boxplot\n",
    "        print('     Исследование  данных в столбцах по типу box-and-wisker plot (ящик с усами)   ')\n",
    "        print('-' * 80)\n",
    "        cols = df.columns\n",
    "        for column in cols:  \n",
    "            print('Гистограмма: ', column )\n",
    "            name_type_hist(data =df, x = column) \n",
    "            plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c2ca0-37e5-4264-a14e-417bcef5bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем функцию plt_hist с модулем sns.histplot печати гистограмм данных в столбцах от количества\n",
    "# задаем параметр bins = 20\n",
    "plt_hist(hw_data_composite_new, sns.histplot, 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d9465-8203-479c-b17a-ac1652f64f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем функцию plt_hist с модулем sns.displot печати гистограмм данных в столбцах от плотности непрерывное случайное распределение\n",
    "plt_hist(hw_data_composite_new, sns.displot, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d1131-a35d-474a-acae-d2a2889060f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Используем функцию plt_hist с модулем sns.displot печати гистограмм данных в столбцах от плотности непрерывное случайное распределение\n",
    "plt_hist(hw_data_composite_new, sns.boxplot, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11f16b-9d7a-437f-b8f6-eb1cc7e4b0ee",
   "metadata": {},
   "source": [
    "# Все дальнейшие действия по изменению данных в  DataFrem \"hw_data_composite_new.csv\" будем проводить в новом Jupiter notebook \"Diploma_2\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
